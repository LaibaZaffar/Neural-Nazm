{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBsaZ-lebh9P",
        "outputId": "b623ba1f-4267-4195-86d8-87dbd26db659"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m1124/1124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.0539 - loss: 6.9861"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1124/1124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 112ms/step - accuracy: 0.0539 - loss: 6.9859 - val_accuracy: 0.0736 - val_loss: 6.5051\n",
            "Epoch 2/50\n",
            "\u001b[1m1124/1124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.0753 - loss: 6.3865"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1124/1124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 111ms/step - accuracy: 0.0754 - loss: 6.3864 - val_accuracy: 0.0810 - val_loss: 6.4333\n",
            "Epoch 3/50\n",
            "\u001b[1m1124/1124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.0883 - loss: 6.1958"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1124/1124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 110ms/step - accuracy: 0.0883 - loss: 6.1958 - val_accuracy: 0.0953 - val_loss: 6.3887\n",
            "Epoch 4/50\n",
            "\u001b[1m1124/1124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.0995 - loss: 6.0382"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1124/1124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 110ms/step - accuracy: 0.0995 - loss: 6.0382 - val_accuracy: 0.0991 - val_loss: 6.3878\n",
            "Epoch 5/50\n",
            "\u001b[1m1124/1124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 111ms/step - accuracy: 0.1075 - loss: 5.9236 - val_accuracy: 0.1042 - val_loss: 6.3920\n",
            "Epoch 6/50\n",
            "\u001b[1m1124/1124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 111ms/step - accuracy: 0.1147 - loss: 5.7884 - val_accuracy: 0.1067 - val_loss: 6.4155\n",
            "Epoch 7/50\n",
            "\u001b[1m1124/1124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 111ms/step - accuracy: 0.1179 - loss: 5.6926 - val_accuracy: 0.1100 - val_loss: 6.4426\n",
            "Epoch 8/50\n",
            "\u001b[1m1124/1124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 111ms/step - accuracy: 0.1228 - loss: 5.5877 - val_accuracy: 0.1107 - val_loss: 6.4895\n",
            "Epoch 9/50\n",
            "\u001b[1m1124/1124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 110ms/step - accuracy: 0.1268 - loss: 5.4704 - val_accuracy: 0.1126 - val_loss: 6.5480\n",
            "Epoch 10/50\n",
            "\u001b[1m1124/1124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 110ms/step - accuracy: 0.1328 - loss: 5.3622 - val_accuracy: 0.1122 - val_loss: 6.6514\n",
            "Epoch 11/50\n",
            "\u001b[1m1124/1124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 109ms/step - accuracy: 0.1355 - loss: 5.2652 - val_accuracy: 0.1148 - val_loss: 6.7986\n",
            "Epoch 12/50\n",
            "\u001b[1m1124/1124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 111ms/step - accuracy: 0.1390 - loss: 5.1601 - val_accuracy: 0.1163 - val_loss: 6.9683\n",
            "Epoch 13/50\n",
            "\u001b[1m1124/1124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 111ms/step - accuracy: 0.1428 - loss: 5.0607 - val_accuracy: 0.1156 - val_loss: 7.1762\n",
            "Epoch 14/50\n",
            "\u001b[1m1124/1124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 110ms/step - accuracy: 0.1468 - loss: 4.9596 - val_accuracy: 0.1171 - val_loss: 7.4493\n",
            "Epoch 15/50\n",
            "\u001b[1m1124/1124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 110ms/step - accuracy: 0.1523 - loss: 4.8612 - val_accuracy: 0.1191 - val_loss: 7.6174\n",
            "Epoch 16/50\n",
            "\u001b[1m1124/1124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 110ms/step - accuracy: 0.1577 - loss: 4.7741 - val_accuracy: 0.1198 - val_loss: 7.9158\n",
            "Epoch 17/50\n",
            "\u001b[1m1124/1124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 110ms/step - accuracy: 0.1645 - loss: 4.6873 - val_accuracy: 0.1167 - val_loss: 8.2007\n",
            "Epoch 18/50\n",
            "\u001b[1m1124/1124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 111ms/step - accuracy: 0.1709 - loss: 4.6022 - val_accuracy: 0.1165 - val_loss: 8.5161\n",
            "Epoch 19/50\n",
            "\u001b[1m1124/1124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 111ms/step - accuracy: 0.1769 - loss: 4.5357 - val_accuracy: 0.1162 - val_loss: 8.7276\n",
            "Epoch 20/50\n",
            "\u001b[1m1124/1124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 111ms/step - accuracy: 0.1843 - loss: 4.4704 - val_accuracy: 0.1158 - val_loss: 9.0316\n",
            "Epoch 21/50\n",
            "\u001b[1m1124/1124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 111ms/step - accuracy: 0.1927 - loss: 4.3925 - val_accuracy: 0.1151 - val_loss: 9.2631\n",
            "Epoch 22/50\n",
            "\u001b[1m1124/1124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 111ms/step - accuracy: 0.2000 - loss: 4.3370 - val_accuracy: 0.1141 - val_loss: 9.4824\n",
            "Epoch 23/50\n",
            "\u001b[1m1124/1124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 111ms/step - accuracy: 0.2049 - loss: 4.2758 - val_accuracy: 0.1120 - val_loss: 9.7951\n",
            "Epoch 24/50\n",
            "\u001b[1m1124/1124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 111ms/step - accuracy: 0.2106 - loss: 4.2296 - val_accuracy: 0.1126 - val_loss: 10.0182\n",
            "Epoch 25/50\n",
            "\u001b[1m1124/1124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 111ms/step - accuracy: 0.2180 - loss: 4.1726 - val_accuracy: 0.1100 - val_loss: 10.2336\n",
            "Epoch 26/50\n",
            "\u001b[1m1124/1124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 110ms/step - accuracy: 0.2253 - loss: 4.1248 - val_accuracy: 0.1106 - val_loss: 10.4491\n",
            "Epoch 27/50\n",
            "\u001b[1m1124/1124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 110ms/step - accuracy: 0.2298 - loss: 4.0807 - val_accuracy: 0.1126 - val_loss: 10.6515\n",
            "Epoch 28/50\n",
            "\u001b[1m1124/1124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 110ms/step - accuracy: 0.2355 - loss: 4.0362 - val_accuracy: 0.1095 - val_loss: 10.8888\n",
            "Epoch 29/50\n",
            "\u001b[1m1124/1124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 110ms/step - accuracy: 0.2419 - loss: 3.9897 - val_accuracy: 0.1059 - val_loss: 11.0190\n",
            "Epoch 30/50\n",
            "\u001b[1m1124/1124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 110ms/step - accuracy: 0.2464 - loss: 3.9570 - val_accuracy: 0.1112 - val_loss: 11.2801\n",
            "Epoch 31/50\n",
            "\u001b[1m1124/1124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 110ms/step - accuracy: 0.2501 - loss: 3.9096 - val_accuracy: 0.1079 - val_loss: 11.4080\n",
            "Epoch 32/50\n",
            "\u001b[1m1124/1124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 110ms/step - accuracy: 0.2586 - loss: 3.8630 - val_accuracy: 0.1067 - val_loss: 11.7160\n",
            "Epoch 33/50\n",
            "\u001b[1m1124/1124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 109ms/step - accuracy: 0.2640 - loss: 3.8139 - val_accuracy: 0.1085 - val_loss: 11.8347\n",
            "Epoch 34/50\n",
            "\u001b[1m1124/1124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 109ms/step - accuracy: 0.2663 - loss: 3.8010 - val_accuracy: 0.1050 - val_loss: 12.0502\n",
            "Epoch 35/50\n",
            "\u001b[1m1124/1124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 110ms/step - accuracy: 0.2761 - loss: 3.7351 - val_accuracy: 0.1062 - val_loss: 12.1711\n",
            "Epoch 36/50\n",
            "\u001b[1m1124/1124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 109ms/step - accuracy: 0.2783 - loss: 3.7118 - val_accuracy: 0.1067 - val_loss: 12.4463\n",
            "Epoch 37/50\n",
            "\u001b[1m1124/1124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 109ms/step - accuracy: 0.2840 - loss: 3.6681 - val_accuracy: 0.1051 - val_loss: 12.6337\n",
            "Epoch 38/50\n",
            "\u001b[1m1124/1124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 109ms/step - accuracy: 0.2889 - loss: 3.6364 - val_accuracy: 0.1022 - val_loss: 12.8536\n",
            "Epoch 39/50\n",
            "\u001b[1m1124/1124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 110ms/step - accuracy: 0.2924 - loss: 3.6021 - val_accuracy: 0.1060 - val_loss: 13.0454\n",
            "Epoch 40/50\n",
            "\u001b[1m1124/1124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 110ms/step - accuracy: 0.2992 - loss: 3.5620 - val_accuracy: 0.1017 - val_loss: 13.1835\n",
            "Epoch 41/50\n",
            "\u001b[1m1124/1124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 109ms/step - accuracy: 0.3059 - loss: 3.5276 - val_accuracy: 0.1011 - val_loss: 13.2539\n",
            "Epoch 42/50\n",
            "\u001b[1m1124/1124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 109ms/step - accuracy: 0.3091 - loss: 3.5007 - val_accuracy: 0.0997 - val_loss: 13.5061\n",
            "Epoch 43/50\n",
            "\u001b[1m1124/1124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 110ms/step - accuracy: 0.3123 - loss: 3.4686 - val_accuracy: 0.1000 - val_loss: 13.6948\n",
            "Epoch 44/50\n",
            "\u001b[1m1124/1124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 109ms/step - accuracy: 0.3199 - loss: 3.4295 - val_accuracy: 0.0989 - val_loss: 13.9510\n",
            "Epoch 45/50\n",
            "\u001b[1m1124/1124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 109ms/step - accuracy: 0.3257 - loss: 3.3886 - val_accuracy: 0.1023 - val_loss: 14.0766\n",
            "Epoch 46/50\n",
            "\u001b[1m1124/1124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 109ms/step - accuracy: 0.3264 - loss: 3.3801 - val_accuracy: 0.0996 - val_loss: 14.2197\n",
            "Epoch 47/50\n",
            "\u001b[1m1124/1124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 109ms/step - accuracy: 0.3333 - loss: 3.3284 - val_accuracy: 0.1001 - val_loss: 14.3174\n",
            "Epoch 48/50\n",
            "\u001b[1m1124/1124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 109ms/step - accuracy: 0.3370 - loss: 3.3059 - val_accuracy: 0.0986 - val_loss: 14.5660\n",
            "Epoch 49/50\n",
            "\u001b[1m1124/1124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 109ms/step - accuracy: 0.3419 - loss: 3.2698 - val_accuracy: 0.0965 - val_loss: 14.8243\n",
            "Epoch 50/50\n",
            "\u001b[1m1124/1124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 109ms/step - accuracy: 0.3470 - loss: 3.2381 - val_accuracy: 0.0990 - val_loss: 14.9833\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📝 Generated Poetry:\n",
            " muj se pehli se mohabbat churā kar chale gayā bastiyo ab shahr e ruḳhsat lagā rakkhā hai miyāñ tire kūchoñ meñ mahbūb e dīdār kar diyā aur kyā raushan ham par jalva rau sahte jā rahā huuñ maiñ kaun thā vo batlāo ki mujh ko chhupā ke luut liyā duhāī hai tire kaash yaañ ki\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load Dataset\n",
        "df = pd.read_csv(\"Roman-Urdu-Poetry.csv\", usecols=[\"Poetry\"])\n",
        "\n",
        "# Clean Text Data\n",
        "def clean_poetry_text(text):\n",
        "    text = text.lower()  # Convert text to lowercase\n",
        "    text = re.sub(r'[^a-zA-ZāčēğīñōūṣṭẓḳḌ -]', '', text)  # Remove unwanted characters\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
        "    return text\n",
        "\n",
        "df[\"Poetry\"] = df[\"Poetry\"].apply(clean_poetry_text)\n",
        "\n",
        "# Tokenizer Setup\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df[\"Poetry\"].tolist())\n",
        "\n",
        "# Convert text to sequences\n",
        "sequences = tokenizer.texts_to_sequences(df[\"Poetry\"].tolist())\n",
        "\n",
        "# Create input-output sequences\n",
        "input_sequences = []\n",
        "for seq in sequences:\n",
        "    for i in range(1, len(seq)):\n",
        "        input_sequences.append(seq[:i + 1])\n",
        "\n",
        "# Padding Sequences\n",
        "max_sequence_length = max(len(seq) for seq in input_sequences)\n",
        "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "# Split into X and Y\n",
        "X, Y = input_sequences[:, :-1], input_sequences[:, -1]\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# LSTM Model Definition\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "embedding_dim = 100\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, input_length=X_train.shape[1]),\n",
        "    LSTM(150, return_sequences=True), #long term dependencies \n",
        "    LSTM(150),\n",
        "    Dense(150, activation=\"relu\"),  \n",
        "    Dense(vocab_size, activation=\"softmax\")  #Predicts the next word using Softmax over vocabulary.\n",
        "])\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=Adam(learning_rate=0.001), metrics=[\"accuracy\"])\n",
        "\n",
        "# Callbacks (disable EarlyStopping for all epochs to train fully)\n",
        "checkpoint = ModelCheckpoint(\"poetry_lstm_model.h5\", save_best_only=True, monitor=\"val_loss\")\n",
        "\n",
        "# Train the Model for 50 epochs\n",
        "history = model.fit(X_train, Y_train, epochs=50, batch_size=128, validation_data=(X_val, Y_val), callbacks=[checkpoint])\n",
        "\n",
        "# Save the Model\n",
        "model.save(\"lstm_poetry_model.h5\")\n",
        "\n",
        "# Generate Poetry Function\n",
        "def generate_poetry(seed_text, next_words=50, temperature=0.8):\n",
        "    generated_text = seed_text.lower()\n",
        "    for _ in range(next_words):\n",
        "        tokenized_input = tokenizer.texts_to_sequences([generated_text])\n",
        "        tokenized_input = pad_sequences(tokenized_input, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "        predicted_probs = model.predict(tokenized_input, verbose=0)[0]\n",
        "\n",
        "        # Apply temperature scaling\n",
        "        predicted_probs = np.log(predicted_probs + 1e-8) / temperature\n",
        "        predicted_probs = np.exp(predicted_probs) / np.sum(np.exp(predicted_probs))  # Normalize\n",
        "\n",
        "        # Sample a word based on the probabilities\n",
        "        predicted_word_index = np.random.choice(len(predicted_probs), p=predicted_probs)\n",
        "        predicted_word = tokenizer.index_word.get(predicted_word_index, \"\")\n",
        "\n",
        "        if not predicted_word:\n",
        "            break\n",
        "\n",
        "        generated_text += \" \" + predicted_word\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# Example Usage\n",
        "seed_text = \"muj se pehli se mohabbat\"\n",
        "generated_poetry = generate_poetry(seed_text)\n",
        "print(\"📝 Generated Poetry:\\n\", generated_poetry)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLQImMdXcGK7",
        "outputId": "08165c72-cc41-4b25-d250-f94c114f47e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📝 Generated Poetry:\n",
            " awal ki dosti hai hāl e aadā kahūñ aur jo dushvār thā patthar ko to sabāt ho yahī kyā moāmla kyā ho ki zindagī ki vo chilman bhī hai lekin aap aaī maiñ aur bhī kyā mire iḳhtiyār pe pahrā chāhiye us meñ baḳhshish e vafā se kyā kiije usī kī ḳhair ab bhī\n"
          ]
        }
      ],
      "source": [
        "seed_text = \"awal ki dosti hai\"\n",
        "generated_poetry = generate_poetry(seed_text)\n",
        "print(\"📝 Generated Poetry:\\n\", generated_poetry)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
